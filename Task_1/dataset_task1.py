# -*- coding: utf-8 -*-
"""dataset-task1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oiz_T--4GI_V9pDw7vWSylvyZP23LETs
"""

pip install bing-image-downloader

from bing_image_downloader import downloader

downloader.download("hawk", limit=100, output_dir='/kaggle/working/', adult_filter_off=True, force_replace=False, timeout=60)

import shutil

shutil.make_archive('/kaggle/working/hawk_images', 'zip', '/kaggle/working/hawk')

downloader.download("swan", limit=100, output_dir='/kaggle/working/', adult_filter_off=True, force_replace=False, timeout=60)

shutil.make_archive('/kaggle/working/swan_images', 'zip', '/kaggle/working/swan')

# Uploaded the downloaded zip files and extracted under dataset section in sidebar of kaggle.

# Creating train-test split for each class of dataset.

import os
import cv2
import numpy as np
import pandas as pd
import tensorflow as tf

dataset_dir='/kaggle/input/task1-dataset/dataset'
class1_dir=os.path.join(dataset_dir,'hawk')
class2_dir=os.path.join(dataset_dir,'swan')

X1=[]
y1=[]
X2=[]
y2=[]

import imghdr
def image_preprocessing(image_paths,target_size=(300,300)):
    processed_images=[]
    for path in image_paths:
        image_format = imghdr.what(path)

        if image_format not in {'jpeg', 'png', 'gif', 'bmp'}:
            print(f"Skipping file {path} due to unsupported format: {image_format}.")
            continue
        img = tf.io.read_file(path)

        try:
            img = tf.image.decode_image(img, channels=3)
            img = tf.image.resize(img, target_size)
            img = tf.cast(img, tf.uint8)  # Adjust if you want to normalize to 0-1
        except tf.errors.InvalidArgumentError:
            print(f"Skipping file {path} due to unsupported format 2.")
            continue
        processed_images.append(img)

    return tf.stack(processed_images)

class1_images=[os.path.join(class1_dir,image) for image in os.listdir(class1_dir)]
X1=image_preprocessing(class1_images,target_size=(300,300))
y1=['hawk']*len(X1)

class2_images=[os.path.join(class2_dir,image) for image in os.listdir(class2_dir)]
X2=image_preprocessing(class2_images,target_size=(300,300))
y2=['swan']*len(X2)

y1=np.array(y1)
y2=np.array(y2)

print(X1[:3])
print(y1)
print(X2[:3])
print(y2)

print(X1.shape)
print(X2.shape)

print(y1.shape)
print(y2.shape)

X=np.concatenate([X1,X2],axis=0)
y=np.concatenate([y1,y2],axis=0)

import sklearn
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test= train_test_split(X,y,test_size=0.2,stratify=y, random_state=42)

from sklearn.utils import shuffle

X_train, y_train=shuffle(X_train, y_train, random_state=42)
X_test, y_test=shuffle(X_test, y_test, random_state=42)

print(X_train[:2],y_train[:2])

print(X_test[:2])

# train_df=pd.DataFrame({'X_train':list(X_train), 'y_train':list(y_train)})
# test_df=pd.DataFrame({'X_test':list(X_test), 'y_test':list(y_test)})

# train_df.iloc[0:2]

np.save('X_train.npy', X_train)
np.save('y_train.npy', y_train)

np.save('X_test.npy', X_test)
np.save('y_test.npy', y_test)

