# -*- coding: utf-8 -*-
"""vgg_models_task1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nLHrdFXUM079kGYB4gbupjyQZ_rDQG5i
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D
from tensorflow.keras.layers import MaxPooling2D
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import BinaryCrossentropy
from tensorflow.keras.metrics import BinaryAccuracy
import time
import io
import os

from google.colab import drive
drive.mount('/content/drive')

X_train=np.load('/content/drive/My Drive/ML_Assignment_4/Datasets/X_train.npy')
y_train=np.load('/content/drive/My Drive/ML_Assignment_4/Datasets/y_train.npy')
X_test=np.load('/content/drive/My Drive/ML_Assignment_4/Datasets/X_test.npy')
y_test=np.load('/content/drive/My Drive/ML_Assignment_4/Datasets/y_test.npy')

# dataset_dir_path='dataset'

# X_train=np.load(os.path.join(dataset_dir_path,'X_train.npy'))
# y_train=np.load(os.path.join(dataset_dir_path,'y_train.npy'))
# X_test=np.load(os.path.join(dataset_dir_path,'X_test.npy'))
# y_test=np.load(os.path.join(dataset_dir_path,'y_test.npy'))

label2bin_mapping={'hawk': 0, 'swan': 1}
bin2label_mapping={v:k for k,v in label2bin_mapping.items()}

y_train = np.vectorize(label2bin_mapping.get)(y_train)
y_test = np.vectorize(label2bin_mapping.get)(y_test)

train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))
test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))

for features, label in train_dataset.take(2):
    print("Features:", features.numpy(), "Label:", label.numpy())

for features, label in test_dataset.take(2):
    print("Features:", features.numpy(), "Label:", label.numpy())

def normalize_data(image,label):
    image=tf.cast(image,tf.float32)/255.0
    return image,label

train_dataset=train_dataset.map(normalize_data)

test_dataset=test_dataset.map(normalize_data)

for features, label in train_dataset.take(2):
    print("Features:", features.numpy(), "Label:", label.numpy())

for features, label in test_dataset.take(2):
    print("Features:", features.numpy(), "Label:", label.numpy())

def data_augmentation(image,label):
    image=tf.image.random_flip_left_right(image)
    image=tf.image.random_brightness(image, max_delta=0.1)
    return image,label

train_dataset=train_dataset.map(data_augmentation)

print(X_train.shape)

"""# Defining the model training functions and parameters"""

batch_size=20

train_batches=train_dataset.batch(batch_size)
test_batches=test_dataset.batch(batch_size)

for x_batch, y_batch in train_dataset.take(1):
    print(x_batch.shape)
    print(y_batch.shape)

num_epochs=35
lr=1e-6

def model_training(model,model_name,model_dir,lr,num_epochs,train_batches,test_batches):
    create_log_dirs(model_dir)
    overall_train_acc=0
    overall_test_acc=0
    overall_train_loss=0
    overall_test_loss=0
    total_train_time=0
    total_params=sum([tf.size(var).numpy() for var in model.trainable_variables])

    train_writer=tf.summary.create_file_writer(f"{model_dir}/train")
    test_writer=tf.summary.create_file_writer(f"{model_dir}/test")
    overall_writer=tf.summary.create_file_writer(f"{model_dir}/overall")

    optimizer=Adam(learning_rate=lr)
    loss_fn=BinaryCrossentropy()
    accuracy=BinaryAccuracy()

    train_step=test_step=step=0

    start_time=time.time()

    for epoch in range(num_epochs):
        epoch_train_loss=0
        epoch_train_acc=0
        batch_train_losses=[]
        batch_train_accs=[]

        epoch_test_loss=0
        epoch_test_acc=0
        batch_test_losses=[]
        batch_test_accs=[]

        for batch_no, (X, y) in enumerate(train_batches):
            with tf.GradientTape() as tape:
                y_pred=model(X, training=True)
                loss=loss_fn(y, y_pred)

            gradients=tape.gradient(loss, model.trainable_weights)
            optimizer.apply_gradients(zip(gradients, model.trainable_weights))
            accuracy.update_state(y, y_pred)

            with train_writer.as_default():
                tf.summary.scalar("Loss", loss, step=train_step)
                tf.summary.scalar("Accuracy", accuracy.result(), step=train_step)
            train_step+=1

            batch_train_accs.append(accuracy.result().numpy())
            batch_train_losses.append(loss.numpy())

        epoch_train_loss=sum(batch_train_losses)/len(batch_train_losses)
        epoch_train_acc=sum(batch_train_accs)/len(batch_train_accs)
        print(f"Epoch-{epoch} loss: {epoch_train_loss}")

        overall_train_loss=epoch_train_loss
        overall_train_acc=epoch_train_acc

        accuracy.reset_state()

        for batch_no, (X, y) in enumerate(test_batches):
            y_pred=model(X, training=False)
            loss=loss_fn(y, y_pred)
            accuracy.update_state(y, y_pred)

            with test_writer.as_default():
                tf.summary.scalar("Loss", loss, step=test_step)
                tf.summary.scalar("Accuracy", accuracy.result(), step=test_step)
            test_step+=1

            batch_test_accs.append(accuracy.result().numpy())
            batch_test_losses.append(loss.numpy())

        epoch_test_loss=sum(batch_test_losses)/len(batch_test_losses)
        epoch_test_acc=sum(batch_test_accs)/len(batch_test_accs)

        overall_test_loss=epoch_test_loss
        overall_test_acc=epoch_test_acc

        accuracy.reset_state()

    end_time=time.time()
    total_time=end_time-start_time

    with overall_writer.as_default():
        tf.summary.scalar("Training Time", total_time, step=0)
        tf.summary.scalar("Total Parameters", total_params, step=0)
        tf.summary.scalar("Overall Training Accuracy", overall_train_acc, step=0)
        tf.summary.scalar("Overall Test Accuracy", overall_test_acc, step=0)
        tf.summary.scalar("Overall Training Loss", overall_train_loss, step=0)
        tf.summary.scalar("Overall Test Loss", overall_test_loss, step=0)

    return (total_time,overall_train_loss, overall_test_loss, overall_train_acc, overall_test_acc, total_params)

def pred_visualization(model, test_batches, model_name, model_dir, bin2label_mapping):
    create_log_dirs(model_dir)
    step=0
    pred_image_writer=tf.summary.create_file_writer(f"{model_dir}/pred_imgs")
    for batch_no, (X,y) in enumerate(test_batches):
        y_pred=model(X,training=False)
        y_pred=tf.round(y_pred)
        y_pred=y_pred.numpy()
        y=y.numpy()

        figure=plt.figure(figsize=(10, 10))
        num_images=X.shape[0]
        size=int(np.ceil(np.sqrt(num_images)))

        for i in range(X.shape[0]):
            img=X[i].numpy()
            true_label=bin2label_mapping[int(y[i])]
            pred_label=bin2label_mapping[int(y_pred[i])]

            plt.subplot(size,size,i+1,title=f'True: {true_label}\nPredicted: {pred_label}')
            plt.axis('off')
            plt.imshow(X[i])
            plt.tight_layout()

        buf = io.BytesIO()
        plt.savefig(buf, format="png")
        plt.close(figure)
        buf.seek(0)

        figure=tf.image.decode_png(buf.getvalue(), channels=4)
        figure=tf.expand_dims(figure, 0)

        with pred_image_writer.as_default():
            tf.summary.image(
                "Predicted Images", figure, step=step,
            )
            step += 1

def create_log_dirs(log_dir):
    os.makedirs(log_dir, exist_ok=True)
    os.makedirs(f"{log_dir}/train", exist_ok=True)
    os.makedirs(f"{log_dir}/test", exist_ok=True)
    os.makedirs(f"{log_dir}/overall", exist_ok=True)
    os.makedirs(f"{log_dir}/pred_imgs", exist_ok=True)

"""# Model: VGG1"""

def build_VGG1():
    model = Sequential()
    model.add(Input(shape=(300, 300, 3)))
    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Flatten())
    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))
    model.add(Dense(1, activation='sigmoid'))

    return model

model_VGG1=build_VGG1()
model_VGG1_name='VGG1'
VGG1_dir='logs/VGG1'

training_time_VGG1, training_loss_VGG1, testing_loss_VGG1, training_acc_VGG1, testing_acc_VGG1, total_params_VGG1 = model_training(model_VGG1,model_VGG1_name, VGG1_dir,lr,num_epochs,train_batches,test_batches)

print(f"Training time for VGG1: {training_time_VGG1}s")
print(f"Training Accuracy for VGG1: {training_acc_VGG1}")
print(f"Test Accuracy for VGG1: {testing_acc_VGG1}")
print(f"Training Loss for VGG1: {training_loss_VGG1}")
print(f"Test Loss for VGG1: {testing_loss_VGG1}")
print(f"Number of model parameters for VGG1: {total_params_VGG1}")

"""## Tesnsor board visualization"""

pred_visualization(model_VGG1, test_batches, model_VGG1_name, VGG1_dir, bin2label_mapping)

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir VGG1_dir

